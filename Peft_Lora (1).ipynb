{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZiSEDABcQY8",
        "outputId": "12260048-ce8e-4bb2-b427-5ae4f7d6c069"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# to load model\n",
        "!pip install -q transformers datasets peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksB_YXFKftFT"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments,Trainer\n",
        "from peft import get_peft_model,LoraConfig,TaskType\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KcmDqiYhQY3"
      },
      "outputs": [],
      "source": [
        "data={\n",
        "    \"text\":[\n",
        "        \"### Question: What is CNN?\\n### Answer:CNN means Convolutional Neural Network.\",\n",
        "        \"### Question: What is RNN?\\n### Answer:RNN means Recurrent Neural Network.\"\n",
        "        \"### Question: what is LSTM?\\n### Answer:LSTM means Long Short Term Memory and it is alos a type of RNN.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "dataset=Dataset.from_dict(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Sxm9zpCXj-W7"
      },
      "outputs": [],
      "source": [
        "model_name=\"tiiuae/falcon-rw-1b\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token=tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1CfFIro8kJTs"
      },
      "outputs": [],
      "source": [
        "def tokenize(batch):\n",
        "  tokens=tokenizer(\n",
        "      batch[\"text\"],\n",
        "      padding=\"max_length\",\n",
        "      truncation=True,\n",
        "      max_length=128 #if more than 128 it will truncate\n",
        "  )\n",
        "  tokens[\"labels\"]=tokens[\"input_ids\"].copy() #sequence (before word)\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "27aa46ddfa474492bc58d0f37f94b811"
          ]
        },
        "id": "3ANQ7y0FlXuE",
        "outputId": "0fdd85ce-38d9-4f85-b8ba-66f405f9edac"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27aa46ddfa474492bc58d0f37f94b811",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#fitiing all the thing from the above\n",
        "tokenized_dataset=dataset.map(tokenize,batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "qIGl0b5xnQDI",
        "outputId": "e96f2997-e532-4e35-e7a7-ee6b79591cdc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        }
      ],
      "source": [
        "#finetuning\n",
        "model=AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\" #hadware config and memory\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gwxqlGwunQGa"
      },
      "outputs": [],
      "source": [
        "#lora configuration for fine tuning\n",
        "lora_config=LoraConfig(\n",
        "    r=8, #no of adapters\n",
        "    lora_alpha=16, #scaling\n",
        "    target_modules=[\"query_key_value\"], #only here lora works(query_key_value-top 1 important in attention mechanism and trains it)\n",
        "    lora_dropout=0.1,  #(While working all chances for overfit-so drop here)\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM #causual language modeling\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "lj0gJEZAqZqI",
        "outputId": "c7c74923-bb1b-4fe2-e0a5-df7c9042860b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,572,864 || all params: 1,313,198,080 || trainable%: 0.1198\n"
          ]
        }
      ],
      "source": [
        "model=get_peft_model(model,lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "g4pNhgqVrR_g"
      },
      "outputs": [],
      "source": [
        "#setting training parameters\n",
        "training_args=TrainingArguments(\n",
        "    output_dir=\".results/\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=1,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "CnY-EhUTsqaS",
        "outputId": "b9640e99-7f76-402c-99f4-27eec802dbbb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-25-3512634422.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer=Trainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 00:00, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>6.279300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.465500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.936600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.889700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>4.106800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.293300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=6, training_loss=4.4951984484990435, metrics={'train_runtime': 1.2065, 'train_samples_per_second': 4.973, 'train_steps_per_second': 4.973, 'total_flos': 5576488648704.0, 'train_loss': 4.4951984484990435, 'epoch': 3.0})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#train using trainer\n",
        "trainer=Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "8yOpz45dtgzF",
        "outputId": "9f526de3-23a0-46cf-e8af-93308721c42e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('lora_finetuned/tokenizer_config.json',\n",
              " 'lora_finetuned/special_tokens_map.json',\n",
              " 'lora_finetuned/vocab.json',\n",
              " 'lora_finetuned/merges.txt',\n",
              " 'lora_finetuned/added_tokens.json',\n",
              " 'lora_finetuned/tokenizer.json')"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"lora-finetuned\")\n",
        "tokenizer.save_pretrained(\"lora_finetuned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NKR4RnqZtPL1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}